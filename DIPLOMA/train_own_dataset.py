import json

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, SubsetRandomSampler
import matplotlib.pyplot as plt
from datetime import datetime
from custom_dataset_loader import CustomDataset
from model_get import get_model
from normalizer import compute_mean_std
import time

start_time = time.time()


# --- –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ---
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.001
IMG_SIZE = 128
STOP_CRITERION = 99 # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –¥–ª—è –∑—É–ø–∏–Ω–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è. -1 - –Ω–µ –ø—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏
# MODEL_TYPE = "mobilenet_v3_small"
MODEL_TYPE = "resnet50"
MODEL_PATH = f"{MODEL_TYPE}_custom_with_metadata.pth"

DATASET_PATH = r"D:\LPNU\DIPLOMA\DIPLOMA\dataset"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


date = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
transform = transforms.Compose([
    transforms.ToTensor()
])


def make_normilize_transform(dataset):
    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç ---

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    if len(dataset) == 0:
        raise ValueError("‚ùå –ü–æ–º–∏–ª–∫–∞: –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –¥–∞–Ω—ñ –∫–æ—Ä–µ–∫—Ç–Ω—ñ.")

    mean, std = compute_mean_std(dataloader)
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    return transform, mean, std


# --- –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏ ---
def train(dataset, mean, std, show_plot = False):
    indices = list(range(len(dataset)))
    train_indices, val_indices = train_test_split(indices, test_size=0.2,
                                                  stratify=[dataset[idx][1].item() for idx in indices])

    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    # --- –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö ---
    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)
    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)
    num_classes = len(dataset.class_to_idx)  # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤

    # üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ


    # üîπ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
    model = get_model(MODEL_TYPE, num_classes).to(DEVICE)

    # --- –§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä ---
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    def evaluate_model(model, dataloader):
        model.eval()
        correct, total = 0, 0

        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        accuracy = correct / total * 100
        return accuracy

    def plot_training_results(train_losses, train_accuracies, val_accuracies, epochs, save_path=f"training_plot {date}.png"):
        """
        –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è.

        :param train_losses: –°–ø–∏—Å–æ–∫ –≤—Ç—Ä–∞—Ç –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ
        :param train_accuracies: –°–ø–∏—Å–æ–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ
        :param val_accuracies: –°–ø–∏—Å–æ–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        :param epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
        :param save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
        """
        plt.figure(figsize=(10, 5))
        plt.plot(range(1, epochs + 1), train_losses, label="–í—Ç—Ä–∞—Ç–∏ (Train)", marker="o")
        plt.plot(range(1, epochs + 1), train_accuracies, label="–¢–æ—á–Ω—ñ—Å—Ç—å (Train)", marker="s")
        plt.plot(range(1, epochs + 1), val_accuracies, label="–¢–æ—á–Ω—ñ—Å—Ç—å (Validation)", marker="^")

        plt.xlabel("–ï–ø–æ—Ö–∏")
        plt.ylabel("–ó–Ω–∞—á–µ–Ω–Ω—è")
        plt.title("–î–∏–Ω–∞–º—ñ–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ")
        plt.legend()
        plt.grid(True)

        plt.savefig(save_path)
        plt.show()

        print(f"üìä –ì—Ä–∞—Ñ—ñ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {save_path}")

    def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):
        train_losses = []
        train_accuracies = []
        val_accuracies = []
        epochs_trained = 0
        for epoch in range(epochs):
            epochs_trained += 1
            model.train()
            running_loss = 0.0
            correct, total = 0, 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

            train_loss = running_loss / len(train_loader)
            train_acc = correct / total * 100
            val_acc = evaluate_model(model, val_loader)

            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)

            print(
                f"üåÄ –ï–ø–æ—Ö–∞ {epoch + 1}/{epochs} | –í—Ç—Ä–∞—Ç–∞: {train_loss:.4f} | –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {train_acc:.2f}% | –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {val_acc:.2f}%")
            # üîπ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∑—É–ø–∏–Ω–∫—É
            if epoch > 0 and val_acc >= STOP_CRITERION and STOP_CRITERION!= -1:
                print(f"üî¥ –ó—É–ø–∏–Ω–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è: –¥–æ—Å—è–≥–Ω—É—Ç–æ –∫—Ä–∏—Ç–µ—Ä—ñ—é –∑—É–ø–∏–Ω–∫–∏. –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å {val_acc}")
                break
        # üîπ –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –ø—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        train_time = time.time() - start_time
        if show_plot:
            plot_training_results(train_losses, train_accuracies, val_accuracies, epochs_trained)
        json_data = {
            "img_size": IMG_SIZE,
            "model_type": MODEL_TYPE,
            "class_names": list(dataset.class_to_idx.keys()),
            "train_time": train_time,
            "train_losses": train_losses,
            "train_accuracies": train_accuracies,
            "val_accuracies": val_accuracies
        }
        with open(f"training_results_{date}.json", "w") as f:
            json.dump(json_data, f, indent=4)
        # üîπ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        metadata = {
            "normalization_mean": list(mean),
            "normalization_std": list(std),
            "img_size": IMG_SIZE,
            "model_type": MODEL_TYPE,
            "class_names": list(dataset.class_to_idx.keys())
        }
        torch.save({
            "model_state_dict": model.state_dict(),
            "metadata": metadata
        }, MODEL_PATH)
        print(f"–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è {train_time}c")
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ {MODEL_PATH}")

    train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)


# --- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ---
if __name__ == "__main__":
    dataset = CustomDataset(root_dir=DATASET_PATH, transform=transform)

    transform, mean, std = make_normilize_transform(dataset)
    dataset.transform = transform
    train(dataset, mean, std)
