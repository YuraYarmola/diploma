import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, SubsetRandomSampler

from custom_dataset_loader import CustomDataset
from model_get import get_model
from normalizer import compute_mean_std

# --- –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ---
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.001
IMG_SIZE = 32

MODEL_TYPE = "mobilenet_v3_small"
MODEL_PATH = f"{MODEL_TYPE}_custom_with_metadata.pth"

DATASET_PATH = r"D:\LPNU\DIPLOMA\DIPLOMA\dataset"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")



transform = transforms.Compose([
    transforms.ToTensor()
])


def make_normilize_transform(dataset):
    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç ---

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    if len(dataset) == 0:
        raise ValueError("‚ùå –ü–æ–º–∏–ª–∫–∞: –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –¥–∞–Ω—ñ –∫–æ—Ä–µ–∫—Ç–Ω—ñ.")

    mean, std = compute_mean_std(dataloader)
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    return transform, mean, std


# --- –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏ ---
def train(dataset):
    indices = list(range(len(dataset)))
    train_indices, val_indices = train_test_split(indices, test_size=0.2,
                                                  stratify=[dataset[idx][1].item() for idx in indices])

    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    # --- –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö ---
    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)
    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)
    num_classes = len(dataset.class_to_idx)  # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤

    # üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ


    # üîπ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
    model = get_model(MODEL_TYPE, num_classes).to(DEVICE)

    # --- –§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä ---
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    def evaluate_model(model, dataloader):
        model.eval()
        correct, total = 0, 0

        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        accuracy = correct / total * 100
        return accuracy

    # --- –§—É–Ω–∫—Ü—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ---
    def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):
        for epoch in range(epochs):
            model.train()
            running_loss = 0.0
            correct, total = 0, 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

            train_acc = correct / total * 100
            val_acc = evaluate_model(model, val_loader)
            print(
                f"üåÄ –ï–ø–æ—Ö–∞ {epoch + 1}/{epochs} | –í—Ç—Ä–∞—Ç–∞: {running_loss:.4f} | –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {train_acc:.2f}% | –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {val_acc:.2f}%")

        metadata = {
            "normalization_mean": list(mean),
            "normalization_std": list(std),
            "img_size": IMG_SIZE,
            "model_type": MODEL_TYPE,
            "class_names": list(dataset.class_to_idx.keys())
        }
        torch.save({
            "model_state_dict": model.state_dict(),
            "metadata": metadata
        }, MODEL_PATH)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ {MODEL_PATH}")

    train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)


# --- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ---
if __name__ == "__main__":
    dataset = CustomDataset(root_dir=DATASET_PATH, transform=transform)

    transform, mean, std = make_normilize_transform(dataset)
    dataset.transform = transform
    train(dataset)
